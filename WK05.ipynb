{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WK05: Transformer Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This code imports the functions we need to run our inference pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Completion\n",
    "\n",
    "Let's use the GPT2 model to create some text completions.\n",
    "\n",
    "We use a pipeline object to run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "  \"text-generation\",\n",
    "  model=\"openai-community/gpt2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a starter sentence to run our model on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOBE = \"To be or not to be, that is the\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generator(TOBE, max_length=64, pad_token_id=0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Model\n",
    "\n",
    "is as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "  \"text-generation\",\n",
    "  model=\"Xenova/llama2.c-stories110M\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun with new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generator(TOBE, max_length=64, pad_token_id=0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One last model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "  \"text-generation\",\n",
    "  model=\"facebook/opt-125m\"\n",
    ")\n",
    "\n",
    "result = generator(TOBE, max_length=64, pad_token_id=0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill-Mask\n",
    "\n",
    "can be used to get the probabilities/scores of different possible words to complete a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filler = pipeline(\n",
    "  \"fill-mask\",\n",
    "  model=\"FacebookAI/xlm-roberta-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = f\"{TOBE} <mask>\"\n",
    "result = filler(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TOBE)\n",
    "\n",
    "for r in result:\n",
    "  word = r[\"token_str\"]\n",
    "  score = round(r[\"score\"], 4)\n",
    "  print(len(TOBE) * \" \", f\"{word}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Sentiment Analysis\n",
    "\n",
    "Classify the _tone_ of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = pipeline(\n",
    "  \"sentiment-analysis\",\n",
    "  model=\"joeddav/distilbert-base-uncased-go-emotions-student\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A wave of elation washed over me, like sunlight breaking through the clouds\"\n",
    "result = analyzer(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on example phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_TEXTS = [\n",
    "  \"What a wonderful day\",\n",
    "  \"OMG my head hurts\",\n",
    "  \"What am I doing here?\"\n",
    "]\n",
    "\n",
    "for t in EXAMPLE_TEXTS:\n",
    "  result = analyzer(t)\n",
    "  print(t, \"->\", result[0][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pass an extra parameter to our pipeline if we want to get scores for all possible sentiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_analyzer = pipeline(\n",
    "  \"sentiment-analysis\",\n",
    "  model=\"joeddav/distilbert-base-uncased-go-emotions-student\",\n",
    "  return_all_scores=True\n",
    ")\n",
    "\n",
    "print(text)\n",
    "result = full_analyzer(text)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other models\n",
    "\n",
    "They each have a slightly different set of _emotions_ they were trained to detect.\n",
    "\n",
    "- [GoEmotion BERT](https://huggingface.co/joeddav/distilbert-base-uncased-go-emotions-student) (27 emotions)\n",
    "- [Roberta](https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion-multilabel-latest) (11 emotions)\n",
    "- [Distil BERT](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion) (6 emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_analyzer = pipeline(\n",
    "  \"sentiment-analysis\",\n",
    "  model=\"cardiffnlp/twitter-roberta-base-emotion-multilabel-latest\",\n",
    "  return_all_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)\n",
    "result = full_analyzer(text)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_analyzer = pipeline(\n",
    "  \"sentiment-analysis\",\n",
    "  model=\"bhadresh-savani/distilbert-base-uncased-emotion\",\n",
    "  return_all_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)\n",
    "result = full_analyzer(text)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Description\n",
    "\n",
    "Describe what's in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_captioner = pipeline(\n",
    "  \"image-to-text\",\n",
    "  model=\"Salesforce/blip-image-captioning-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = Image.open(\"./imgs/GDTM.jpg\").convert(\"RGB\")\n",
    "display(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = img_captioner(test_image)\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other image description models:\n",
    "\n",
    "- [`VIT`](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning)\n",
    "- [`LLAVA`](https://huggingface.co/llava-hf/llava-interleave-qwen-0.5b-hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_captioner = pipeline(\n",
    "  \"image-to-text\",\n",
    "  model=\"nlpconnect/vit-gpt2-image-captioning\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vit_captioner(test_image)\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_captioner = pipeline(\n",
    "  \"image-text-to-text\",\n",
    "  model=\"llava-hf/llava-interleave-qwen-0.5b-hf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [{\n",
    "  \"role\": \"user\",\n",
    "  \"content\": [\n",
    "    {\"type\": \"image\", \"image\": test_image},\n",
    "    {\"type\": \"text\", \"text\": \"Describe the image\"},\n",
    "  ],\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llava_captioner(text=prompt, max_new_tokens=32, return_full_text=False)\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth Prediction\n",
    "\n",
    "Some models don't work with the pipeline inference object, but the Transformers library still has some consistent-ish interfaces for running these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "\n",
    "from WK05_utils import DepthAnythingPostProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH_MODEL = \"depth-anything/Depth-Anything-V2-Base-hf\"\n",
    "depth_processor = AutoImageProcessor.from_pretrained(DEPTH_MODEL)\n",
    "depth_model = AutoModelForDepthEstimation.from_pretrained(DEPTH_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image for the model\n",
    "image = Image.open(\"./imgs/flowers.jpg\")\n",
    "depth_inputs = depth_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# run model\n",
    "depth_output = depth_model(**depth_inputs)\n",
    "print(depth_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate to original size\n",
    "depth_image = DepthAnythingPostProcessor.process_output(depth_output, image)\n",
    "display(depth_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection\n",
    "\n",
    "Detect where a set of pre-defined objects are in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJ_MODEL = \"facebook/detr-resnet-101\"\n",
    "detr_processor = DetrImageProcessor.from_pretrained(OBJ_MODEL, revision=\"no_timm\")\n",
    "detr_model = DetrForObjectDetection.from_pretrained(OBJ_MODEL, revision=\"no_timm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./imgs/street.jpg\")\n",
    "detr_inputs = detr_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "detr_output = detr_model(**detr_inputs)\n",
    "detr_results = detr_processor.post_process_object_detection(detr_output, 0.9, [image.size[::-1]])[0]\n",
    "print(detr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detr_slb = zip(detr_results[\"scores\"], detr_results[\"labels\"], detr_results[\"boxes\"])\n",
    "\n",
    "for score, label, box in detr_slb:\n",
    "  print(\n",
    "    f\"Detected {detr_model.config.id2label[label.item()]}\",\n",
    "    f\"with confidence {round(score.item(), 3)}\",\n",
    "    f\"at location {[round(i, 2) for i in box.tolist()]}\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Segmentation\n",
    "\n",
    "Not only detects where objects are, but gives a mask of all pixels that belong to the objects detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
    "\n",
    "from WK05_utils import SegformerPostProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEG_MODEL = \"nvidia/segformer-b1-finetuned-ade-512-512\"\n",
    "seg_processor = SegformerFeatureExtractor.from_pretrained(SEG_MODEL)\n",
    "seg_model = SegformerForSemanticSegmentation.from_pretrained(SEG_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./imgs/street.jpg\")\n",
    "seg_inputs = seg_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "seg_output = seg_model(**seg_inputs)\n",
    "print(seg_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_image = SegformerPostProcessor.process_output(seg_output, image)\n",
    "blended_image = Image.blend(image, segments_image, alpha=0.5)\n",
    "\n",
    "display(segments_image)\n",
    "display(blended_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Modal Image Comprehension\n",
    "\n",
    "Zero-Shot Classification.\n",
    "\n",
    "Detects presence of objects that weren't part of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_MODEL = \"openai/clip-vit-large-patch14\"\n",
    "clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL)\n",
    "clip_model = CLIPModel.from_pretrained(CLIP_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\n",
    "  \"cat\",\n",
    "  \"dog\",\n",
    "  \"bird\",\n",
    "  \"fish\",\n",
    "  \"aquatic mammal\",\n",
    "  \"erinaceinae\",\n",
    "  \"vegetation\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./imgs/arara.jpg\")\n",
    "clip_inputs = clip_processor(text=LABELS, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "clip_output = clip_model(**clip_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_idx = clip_output.logits_per_image.argmax(dim=1)\n",
    "LABELS[label_idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
