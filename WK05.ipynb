{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WK05: Transformer Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This code imports the functions we need to run our inference pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Completion\n",
    "\n",
    "Let's use the GPT2 model to create some text completions.\n",
    "\n",
    "We use a pipeline object to run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "  \"text-generation\",\n",
    "  model=\"openai-community/gpt2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a starter sentence to run our model on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOBE = \"To be or not to be, that is the\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generator(TOBE, max_length=64, pad_token_id=0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Model\n",
    "\n",
    "is as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "  \"text-generation\",\n",
    "  model=\"Xenova/llama2.c-stories110M\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun with new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generator(TOBE, max_length=64, pad_token_id=0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "  \"text-generation\",\n",
    "  model=\"facebook/opt-125m\"\n",
    ")\n",
    "\n",
    "result = generator(TOBE, max_length=64, pad_token_id=0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill-Mask\n",
    "\n",
    "can be used to get the probabilities/scores of different possible words to complete a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filler = pipeline(\n",
    "  \"fill-mask\",\n",
    "  model=\"FacebookAI/xlm-roberta-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = f\"{TOBE} <mask>\"\n",
    "result = filler(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TOBE)\n",
    "\n",
    "for r in result:\n",
    "  word = r[\"token_str\"]\n",
    "  score = round(r[\"score\"], 4)\n",
    "  print(len(TOBE) * \" \", f\"{word}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Sentiment Analysis\n",
    "\n",
    "Define model and pipeline object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = pipeline(\n",
    "  \"sentiment-analysis\",\n",
    "  model=\"joeddav/distilbert-base-uncased-go-emotions-student\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A wave of elation washed over me, like sunlight breaking through the clouds\"\n",
    "result = analyzer(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on example phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_TEXTS = [\n",
    "  \"What a wonderful day\",\n",
    "  \"OMG my head hurts\",\n",
    "  \"What am I doing here?\"\n",
    "]\n",
    "\n",
    "for t in EXAMPLE_TEXTS:\n",
    "  result = analyzer(t)\n",
    "  print(t, \"->\", result[0][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pass an extra parameter to our pipeline if we want to get scores for all possible sentiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_analyzer = pipeline(\n",
    "  \"sentiment-analysis\",\n",
    "  model=\"joeddav/distilbert-base-uncased-go-emotions-student\",\n",
    "  return_all_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = EXAMPLE_TEXTS[0]\n",
    "result = full_analyzer(t)\n",
    "print(t, \":\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New model definition/location and pipeline object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_captioner = pipeline(\n",
    "  \"image-to-text\",\n",
    "  model=\"Salesforce/blip-image-captioning-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = Image.open(\"./imgs/GDTM.jpg\").convert(\"RGB\")\n",
    "display(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = img_captioner(test_image)\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other image description models:\n",
    "- [`LLAVA`](https://huggingface.co/llava-hf/llava-interleave-qwen-0.5b-hf)\n",
    "- [`VIT`](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_captioner = pipeline(\n",
    "  \"image-to-text\",\n",
    "  model=\"llava-hf/llava-interleave-qwen-0.5b-hf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llava_captioner(test_image)\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_captioner = pipeline(\n",
    "  \"image-to-text\",\n",
    "  model=\"nlpconnect/vit-gpt2-image-captioning\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vit_captioner(test_image)\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ways of running inference\n",
    "\n",
    "Some models don't work with the pipeline inference object, but the Transformers library still has some consistent-ish interfaces for running these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH_MODEL = \"depth-anything/Depth-Anything-V2-Base-hf\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(DEPTH_MODEL)\n",
    "model = AutoModelForDepthEstimation.from_pretrained(DEPTH_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image for the model\n",
    "image = Image.open(\"./imgs/flowers\")\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# run model\n",
    "outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate to original size\n",
    "prediction = torch.nn.functional.interpolate(\n",
    "  outputs.predicted_depth.unsqueeze(1),\n",
    "  size=image.size[::-1],\n",
    "  mode=\"bicubic\",\n",
    "  align_corners=False,\n",
    ")\n",
    "\n",
    "# visualize the prediction\n",
    "output = prediction.squeeze().cpu().detach().numpy()\n",
    "formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "depth = Image.fromarray(formatted)\n",
    "\n",
    "display(depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJ_MODEL = \"facebook/detr-resnet-101\"\n",
    "processor = DetrImageProcessor.from_pretrained(OBJ_MODEL, revision=\"no_timm\")\n",
    "model = DetrForObjectDetection.from_pretrained(OBJ_MODEL, revision=\"no_timm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./street.jpg\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "output = model(**inputs)\n",
    "results = processor.post_process_object_detection(output, 0.9, [image.size[::-1]])[0]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "  print(\n",
    "    f\"Detected {model.config.id2label[label.item()]}\",\n",
    "    f\"with confidence {round(score.item(), 3)}\",\n",
    "    f\"at location {[round(i, 2) for i in box.tolist()]}\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
    "\n",
    "from WK05_utils import ADE20K_PALETTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEG_MODEL = \"nvidia/segformer-b1-finetuned-ade-512-512\"\n",
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(SEG_MODEL)\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(SEG_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./imgs/street.jpg\")\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "output = model(**inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.nn.functional.interpolate(\n",
    "    output.logits,\n",
    "    size=image.size[::-1],\n",
    "    mode=\"bicubic\",\n",
    "    align_corners=False\n",
    ")\n",
    "\n",
    "prediction = torch.argmax(logits, dim=1) + 1\n",
    "\n",
    "seg_img = Image.fromarray(prediction.squeeze().cpu().numpy().astype(\"uint8\"))\n",
    "seg_img.putpalette(ADE20K_PALETTE)\n",
    "\n",
    "out_img = Image.blend(image, seg_img.convert(\"RGB\"), alpha=0.5)\n",
    "\n",
    "display(seg_img)\n",
    "display(out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Modal Image Comprehension\n",
    "\n",
    "Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_MODEL = \"openai/clip-vit-large-patch14\"\n",
    "model = CLIPModel.from_pretrained(CLIP_MODEL)\n",
    "processor = CLIPProcessor.from_pretrained(CLIP_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\n",
    "  \"cat\",\n",
    "  \"dog\",\n",
    "  \"bird\",\n",
    "  \"fish\",\n",
    "  \"aquatic mammal\",\n",
    "  \"erinaceinae\",\n",
    "  \"vegetation\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./imgs/arara.jpg\")\n",
    "inputs = processor(text=LABELS, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "output = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_idx = output.logits_per_image.argmax(dim=1)\n",
    "LABELS[label_idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
