{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WK05: Transformer Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This code imports the functions we need to run our inference pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Completion\n",
    "\n",
    "Let's use the GPT2 model to create some text completions.\n",
    "\n",
    "We'll use a pipeline object to run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_GEN_MODEL = \"openai-community/gpt2\"\n",
    "generator = pipeline(\"text-generation\", model=TEXT_GEN_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a starter sentence to run our model on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starter = \"To be or not to be, that is \"\n",
    "result = generator(starter, max_length=64, pad_token_id=0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on many phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_TEXTS = [\n",
    "  \"How much wood would a woodchuck chuck if \",\n",
    "  \"I once knew a man from Natucket, who \",\n",
    "  \"It was the best of times, it was the \"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Model\n",
    "\n",
    "is as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_GEN_MODEL = \"Xenova/llama2.c-stories110M\"\n",
    "generator = pipeline(\"text-generation\", model=TEXT_GEN_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun with new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generator(starter, max_length=64, pad_token_id=0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_GEN_MODEL = \"facebook/opt-125m\"\n",
    "generator = pipeline(\"text-generation\", model=TEXT_GEN_MODEL)\n",
    "\n",
    "result = generator(starter, max_length=64, pad_token_id=0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Sentiment Analysis\n",
    "\n",
    "Define model and pipeline object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_SENT_MODEL = \"joeddav/distilbert-base-uncased-go-emotions-student\"\n",
    "analyzer = pipeline(\"sentiment-analysis\", model=TEXT_SENT_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"A wave of elation washed over me, like sunlight breaking through the clouds\"\n",
    "result = analyzer(test_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on example phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_TEXTS = [\n",
    "  \"What a wonderful day\",\n",
    "  \"OMG my head hurts\",\n",
    "  \"What am I doing here?\"\n",
    "]\n",
    "\n",
    "for t in EXAMPLE_TEXTS:\n",
    "  result = analyzer(t)\n",
    "  print(t, \"->\", result[0][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define our pipeline like this if we want to get scores for all possible sentiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_analyzer = pipeline(\"sentiment-analysis\", model=TEXT_SENT_MODEL, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = EXAMPLE_TEXTS[0]\n",
    "result = full_analyzer(t)\n",
    "print(t, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New model definition/location and pipeline object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_CAP_MODEL = \"Salesforce/blip-image-captioning-base\"\n",
    "img_captioner = pipeline(task=\"image-to-text\", model=IMAGE_CAP_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = Image.open(\"./imgs/GDTM.jpg\").convert(\"RGB\")\n",
    "display(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = img_captioner(test_image)\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other image description models:\n",
    "- [`LLAVA`](https://huggingface.co/llava-hf/llava-interleave-qwen-0.5b-hf)\n",
    "- [`VIT`](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ways of running inference\n",
    "\n",
    "Some models don't work with the pipeline inference object, but the Transformers library still has some consistent-ish interfaces for running these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH_MODEL = \"depth-anything/Depth-Anything-V2-Base-hf\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(DEPTH_MODEL)\n",
    "model = AutoModelForDepthEstimation.from_pretrained(DEPTH_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image for the model\n",
    "image = Image.open(\"./imgs/flowers\")\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# run model\n",
    "outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate to original size\n",
    "prediction = torch.nn.functional.interpolate(\n",
    "  outputs.predicted_depth.unsqueeze(1),\n",
    "  size=image.size[::-1],\n",
    "  mode=\"bicubic\",\n",
    "  align_corners=False,\n",
    ")\n",
    "\n",
    "# visualize the prediction\n",
    "output = prediction.squeeze().cpu().detach().numpy()\n",
    "formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "depth = Image.fromarray(formatted)\n",
    "\n",
    "display(depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJ_MODEL = \"facebook/detr-resnet-101\"\n",
    "processor = DetrImageProcessor.from_pretrained(OBJ_MODEL, revision=\"no_timm\")\n",
    "model = DetrForObjectDetection.from_pretrained(OBJ_MODEL, revision=\"no_timm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./street.jpg\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "output = model(**inputs)\n",
    "results = processor.post_process_object_detection(output, 0.9, [image.size[::-1]])[0]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "  print(\n",
    "    f\"Detected {model.config.id2label[label.item()]}\",\n",
    "    f\"with confidence {round(score.item(), 3)}\",\n",
    "    f\"at location {[round(i, 2) for i in box.tolist()]}\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
    "\n",
    "from WK05_utils import ADE20K_PALETTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEG_MODEL = \"nvidia/segformer-b1-finetuned-ade-512-512\"\n",
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(SEG_MODEL)\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(SEG_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"\")\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "output = model(**inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.nn.functional.interpolate(\n",
    "    output.logits,\n",
    "    size=image.size[::-1],\n",
    "    mode=\"bicubic\",\n",
    "    align_corners=False\n",
    ")\n",
    "\n",
    "prediction = torch.argmax(logits, dim=1) + 1\n",
    "\n",
    "seg_img = Image.fromarray(prediction.squeeze().cpu().numpy().astype(\"uint8\"))\n",
    "seg_img.putpalette(ADE20K_PALETTE)\n",
    "\n",
    "out_img = Image.blend(image, seg_img.convert(\"RGB\"), alpha=0.5)\n",
    "\n",
    "display(seg_img)\n",
    "display(out_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
